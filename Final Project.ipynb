{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Superclass (Done?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        Classifier fitting function\n",
    "            X_train: the features\n",
    "            Y_train: the labels\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Classifier prediction function; to be filled in individually for each classifier\n",
    "        If the subclass has no prediction function, raise an error\n",
    "        \"\"\"\n",
    "        raise RuntimeError(\"This classifier does not have a prediction function\")\n",
    "\n",
    "    def score(self, X_pred, Y_pred):\n",
    "        \"\"\"\n",
    "        Classifier score function.\n",
    "            X_pred: Feature vectors in training set\n",
    "            Y_pred: Corresponding labels for X_pred\n",
    "        \"\"\"\n",
    "        return np.mean(np.equal(self.predict(X_pred), Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 1 - K-Nearest Neighbors (Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNclassifier(Classifier):\n",
    "    def __init__(self, k=2):\n",
    "        self.k = k\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        k-NN prediction function. Adapted from code I had previously written in HW6\n",
    "            X_pred: Feature vectors in training set.\n",
    "        Return the predicted labels for X_pred. Shape: (len(X_pred), )\n",
    "        \"\"\"\n",
    "        Y_pred = []\n",
    "        \n",
    "        #calculate distances\n",
    "        distances = np.linalg.norm(self.X_train[:, np.newaxis] - X, axis = 2)\n",
    "        \n",
    "        #append labels to each distance\n",
    "        labels = np.tile(self.Y_train, (distances.shape[1],1)).T\n",
    "        labeled_distances = np.stack((distances,labels), axis =2)\n",
    "    \n",
    "        #Sort Array based on Distances and find the k closest\n",
    "        sort_indexes = np.argsort(distances,axis = 0)[0:self.k].T\n",
    "        for indeces in sort_indexes:\n",
    "            y_vals = []\n",
    "            for index in indeces:\n",
    "                y_vals.append(self.Y_train[index])\n",
    "            Y_pred.append(stats.mode(y_vals)[0][0])\n",
    "        \n",
    "        return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFClassifier(Classifier):\n",
    "    def __init__(self, depth, trees):\n",
    "        self.depth = depth\n",
    "        self.trees = trees\n",
    "#    def predict(self, X_test):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdsfg.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"gdsfg\"\n",
    "my_str + \".txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 3 - Support Vector Machine (Done w/ linear; still need rbf implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier(Classifier):\n",
    "    def __init__(self, kernel = 'linear',lmda = 5):\n",
    "        valid_kernels = ['linear', 'rbf']\n",
    "        if kernel not in valid_kernels:\n",
    "            raise ValueError('bad kernel')\n",
    "        self.kernels = kernel\n",
    "        self.lmda = lmda\n",
    "            \n",
    "    def fit(self, X_train, Y_train, eta = 0.00005,threshold = 0.00000001, itr = 1000, filename = \"svm_test.txt\"):\n",
    "        \"\"\"\n",
    "        SVM fitting function. Computes the optimal value of theta and stores it as a parameter of the object\n",
    "            X_train: Feature vectors in training set.\n",
    "            Y_train: Labels in training set.\n",
    "            eta: learning rate. Initially set to be 0.05\n",
    "            threshold: point at which the new theta and old theta differ to stop stop iteration\n",
    "            itr: maximum number of iterations\n",
    "        \"\"\"\n",
    "        tic = time.time()\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        f= open(filename,\"w+\")\n",
    "        \n",
    "        #initialize theta* as a random matrix\n",
    "        self.theta_star = np.random.random(X_train.shape[1])\n",
    "        \n",
    "        def h(x,y,theta):\n",
    "            \"\"\"\n",
    "            helper function to calculate the above h(x_i) term in the gradient\n",
    "                x: the i-th vector\n",
    "                y: the label of x\n",
    "                theta: paramter to be optimized\n",
    "            \"\"\"\n",
    "            #if y*x.dot(theta) >= 1:\n",
    "            #    return 0\n",
    "            #else:\n",
    "            #    return -y*x\n",
    "            \n",
    "            # Create a Sparse Diagonal Matrix whose entries correspond to 1 if y*<x,theta> >=1 and 0 else\n",
    "            # That sparse matrix is then used to \"zero out\" any vectors that have low loss value\n",
    "            # That new matrix is then summed over columnwise\n",
    "            S = (((np.sign(y*x.dot(theta)-1)-1)/-2))\n",
    "            S = sp.sparse.diags(S)\n",
    "            S = -S*y\n",
    "            return np.sum(S.dot(x),axis=0)\n",
    "        \n",
    "        #def SVM_grad(X,Y,theta):\n",
    "        #    \"\"\"\n",
    "        #    helper function to calculate the gradient of the SVM loss function with respect to theta\n",
    "        #        X: collection of vectors\n",
    "        #        Y: collection of labels\n",
    "        #        theta: parameter to be optimized\n",
    "        #    \"\"\"\n",
    "        #    return 2*theta + self.lmda*h(X,Y,theta)\n",
    "        \n",
    "        \n",
    "        # Lambda function to \n",
    "        gradient = lambda X,Y,theta: 2*theta + self.lmda*h(X,Y,theta)\n",
    "        \n",
    "        for i in range(itr):\n",
    "#            new_theta = self.theta_star - eta*SVM_grad(self.X_train, self.Y_train, self.theta_star)\n",
    "            new_theta = self.theta_star - eta*gradient(self.X_train, self.Y_train, self.theta_star)\n",
    "            if np.linalg.norm(new_theta - self.theta_star, ord = 1) < threshold:\n",
    "                print('broke at iteration ' + str(i))\n",
    "                break\n",
    "            self.theta_star = new_theta\n",
    "            #normalizing theta to prevent overflow\n",
    "            #self.theta_star = self.theta_star/np.linalg.norm(self.theta_star)\n",
    "        toc = time.time()\n",
    "        f.write(\"lambda = {}\\neta = {}\\nw = {}\\ntraining accuracy is = {}\\nApproximate time to run was : {}\\nSize of data was {}\".format(self.lmda, eta, self.theta_star,self.score(self.X_train, self.Y_train), (toc-tic), self.X_train.shape))\n",
    "        \n",
    "    def predict(self, X_Val):\n",
    "        \"\"\"\n",
    "        SVM prediction function.\n",
    "            X_Val: Feature vectors in training set.\n",
    "        Return the predicted labels for X_pred. Shape: (len(X_Val), )\n",
    "        \"\"\"\n",
    "        Y_pred = []\n",
    "        \n",
    "        for x in X_Val:\n",
    "            pred = 2*int((x.dot(self.theta_star)).astype(np.float64) > 0)-1\n",
    "            Y_pred.append(pred)\n",
    "            \n",
    "        return np.array(Y_pred)\n",
    "    \n",
    "    def normalize_theta(self):\n",
    "        self.theta_star = self.theta_star/np.linalg.norm(self.theta_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 4 - Linear Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-187-2104fa6adcb4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-187-2104fa6adcb4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Boosting(Classifier):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 5 - Single-Layer Perceptron (Some issues still)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(Classifier):\n",
    "    def __init__(self, epochs = 2):\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train#np.c_[np.ones(X_train.shape[0]),X_train]\n",
    "        self.Y_train = Y_train\n",
    "        self.w = np.zeros(self.X_train.shape[1])\n",
    "#        tuned = False\n",
    "#        max_length = self.X_train.shape[0]**2\n",
    "#        i = 0\n",
    "        \n",
    "#        while not tuned and i < max_length:\n",
    "        for i in range(self.epochs):\n",
    "            for x,y in zip(self.X_train, self.Y_train):\n",
    "                if np.sign(x.dot(self.w)) != y:\n",
    "                    new_w = self.w + 2*y*x\n",
    "                    self.w = new_w\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        Y_pred = []\n",
    "        \n",
    "        for x in X_val:\n",
    "            pred = 2*int((x.dot(self.w)).astype(np.float64) > 0)-1\n",
    "            Y_pred.append(pred)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier 6 - Logistic Regression (maybe not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(Classifier):\n",
    "    def fit(X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        self.w = np.zeros(self.X_train.shape[1])\n",
    "        self.b = 0\n",
    "        \n",
    "        def logistic_prob(x,y,W,b):\n",
    "            return 1/(1+np.exp( (-2*y+1)*(W.T.dot(x)+b) ))\n",
    "        \n",
    "        def logistic_loss_gradients(X,Y,W,b):\n",
    "    \n",
    "            P = logistic_prob(X.T, Y.T, w, b).T\n",
    "            #w_grad = \n",
    "            for x,y in zip(X,Y):\n",
    "                p = logistic_prob(x,y,w,b).T\n",
    "                \n",
    "            #w_grad = X.dot(Y-P.dot(X))\n",
    "            #b_grad = Y-P.dot(X)\n",
    "    \n",
    "            return w_grad.T, np.float64(b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning Functions (CV and GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidation(X, Y, Classifier, fold = 3):\n",
    "    #split data into k partitions\n",
    "    Y_fold = np.array_split(Y,fold)\n",
    "    X_fold = np.array_split(X,fold)\n",
    "    \n",
    "    #defining accuracies before going into loop\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for i in range(fold):\n",
    "        \n",
    "        #taking one value out for validation\n",
    "        indeces = list(np.linspace(0, fold-1,fold).astype(int))\n",
    "        indeces.remove(i)\n",
    "        X_val = X_fold[i]\n",
    "        Y_val = Y_fold[i]\n",
    "        \n",
    "        X_train = np.vstack((x for j,x in enumerate(X_fold) if j!=i))\n",
    "        Y_train = np.hstack((y for j,y in enumerate(Y_fold) if j!=i))\n",
    "        \n",
    "        classifier.fit(X_train, Y_train)\n",
    "        train_acc.append(classifier.score(X_train, Y_train))\n",
    "        val_acc.append(classifier.score(X_val, Y_val))\n",
    "    \n",
    "    return np.mean(train_acc), np.mean(val_acc)\n",
    "    \n",
    "\n",
    "def GridSearch(X,Y, Classifier, param_list, folds = 3):\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "#    np.random.seed(0)\n",
    "#    np.random.shuffle(Data)\n",
    "#    X = Data.T[0:len(Data.T)-2].T\n",
    "#    Y = Data.T[len(Data.T)-1]\n",
    "    for param in param_list:\n",
    "        a,b = CrossValidation(X,Y, Classifier, par = param, fold = folds)\n",
    "        train_accs.append(a)\n",
    "        val_accs.append(b)\n",
    "    return np.matrix(train_accs).T, np.matrix(val_accs).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 - Dota 2 data from UCI ML repo\n",
    "Dota_Train = np.genfromtxt('./dota2Dataset/dota2Train.csv', delimiter=',')\n",
    "Dota_Test = np.genfromtxt('./dota2Dataset/dota2Test.csv', delimiter=',')\n",
    "\n",
    "Dota = np.vstack((Dota_Train,Dota_Test))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(Dota)\n",
    "Dota_X = Dota.T[4:].T\n",
    "Dota_Y = Dota.T[0]\n",
    "Ones_Dota_X = np.c_[ np.ones(Dota_X.shape[0]), Dota_X ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2 - Skin data from UCI ML repo\n",
    "Skin = np.loadtxt('Skin_NonSkin.txt')\n",
    "# Mapping labels from {1,2} (Skin, Not Skin) to {1,-1} (Skin, Not Skin)\n",
    "Skin[:,3] = -2*Skin[:,3]+3\n",
    "\n",
    "np.random.seed(4)\n",
    "np.random.shuffle(Skin)\n",
    "Skin_X = Skin.T[0:-1].T\n",
    "Skin_Y = Skin.T[-1]\n",
    "Ones_Skin_X = np.c_[ np.ones(Skin_X.shape[0]), Skin_X ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 200., 198., 164.],\n",
       "       [  1., 129., 166., 218.],\n",
       "       [  1., 146., 148.,  96.],\n",
       "       ...,\n",
       "       [  1., 200., 198., 158.],\n",
       "       [  1.,  41.,  71.,  52.],\n",
       "       [  1., 195., 193., 159.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ones_Skin_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Skin_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  0., -1., ...,  0.,  0.,  0.],\n",
       "       [ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 2.,  0., -1., ...,  1.,  0.,  0.],\n",
       "       [ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 3.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dota[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0., -1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0., -1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ones_Dota_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n",
       "       -1, -1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1,\n",
       "       -1,  1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,\n",
       "        1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1,\n",
       "        1, -1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 200\n",
    "\n",
    "svm2 = SVMClassifier(lmda=5)\n",
    "svm2.fit(Ones_Dota_X[0:n],Dota_Y[0:n])\n",
    "svm2.predict(Ones_Dota_X[0:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2.score(Ones_Dota_X[0:n],Dota_Y[0:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron = Perceptron()\n",
    "ptron.fit(Ones_Dota_X,Dota_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5428096829344109"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.score(Ones_Dota_X,Dota_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ptron.predict(Ones_Dota_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2 = SVMClassifier()\n",
    "svm2.fit(Ones_Skin_X,Skin_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2.normalize_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00480062, -0.61446193, -0.61843088, -0.48985379])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2.theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7924605295910747"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2.score(Ones_Skin_X,Skin_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7924605295910747"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((Skin_Y-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -2.,  0.,  0.,  0.,  0.,  0., -2.,  0.,  2.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -2.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0., -2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,\n",
       "        0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0., -2.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptron.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat=np.random.random(Ones_Dota_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102944, 114)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ones_Dota_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0., -1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
       "        0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ones_Dota_X[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ptron.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(svm2.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289, 0.90483289, 0.90483289,\n",
       "       0.90483289, 0.90483289, 0.90483289])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm2.theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3 - Occupancy Data from UCI ML repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param list for KNN\n",
    "k_list = [1,2,3,4,5]\n",
    "knn_params = {\"k\" : k_list}\n",
    "\n",
    "#param list for perceptron\n",
    "epoch_list = [1,2,4,8,16]\n",
    "perceptron_params = {\"epochs\" : epoch_list}\n",
    "\n",
    "#param list for SVM\n",
    "lambda_list = [1,2,3,4,5]\n",
    "svm_params = {\"lambda\" : lambda_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.476278364936276"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  81,  80,  79,  78,  77,  76,  75,  74,  73,  72,  71,  82,\n",
       "        70,  68,  67,  66,  65,  64,  63,  62,  61,  60,  59,  58,  69,\n",
       "        83,  84,  85, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101,\n",
       "       100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,  89,  88,\n",
       "        87,  86,  57, 111,  56,  54,  24,  23,  22,  21,  20,  19,  18,\n",
       "        17,  16,  15,  14,  25,  13,  11,  10,   9,   8,   7,   6,   5,\n",
       "         4,   3,   2,   1,  12,  26,  27,  28,  53,  52,  51,  50,  49,\n",
       "        48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,  37,  36,\n",
       "        35,  34,  33,  32,  31,  30,  29,  55, 112])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.abs(svm.theta_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9048328935585462"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.theta_star[81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at Highest Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "myFunction() got an unexpected keyword argument 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-64646e78fdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkeyparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'someParam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyparam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmyFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: myFunction() got an unexpected keyword argument 'a'"
     ]
    }
   ],
   "source": [
    "def myFunction(someParam=5):\n",
    "    return someParam\n",
    "\n",
    "keyparam = {'someParam':6}\n",
    "for a in keyparam:\n",
    "    myFunction(a = keyparam[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyparam['someParam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
